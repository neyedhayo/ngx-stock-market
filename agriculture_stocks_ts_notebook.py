# -*- coding: utf-8 -*-
"""agriculture-stocks-ts-notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WonbFIj7vlFqLBZ1Waw1XoqN6mTunalb

<h1><u>AGRICULTURE STOCKS SECTOR</u></h1>

# INTRO

## references
https://pypi.org/project/holidays/ <br>
https://facebook.github.io/prophet/docs/diagnostics.html

## (General steps) Road to pipeline
* Import neccessary libraries
    * define useful functions - (no missing values)
    * read in data
<br><br>
* filter sector-wise data
* preprocess the filtered data
    * plot sector-wise visualization
    * check for missing data
    * clean outliers
    * sector-wise groupings by volume (low/mid/high)
<br><br>
* modelling
    * hyperparameters/tunings
    * cross validation
    * feature engineering
    * prediction backtesting
<br><br>
* forecasting
    * time series pipeline

# IMPORT LIBRARIES & READ DATA

## google drive mount to colab
"""

# connect to gdrive first
# from google.colab import drive
# drive.mount('/content/gdrive')

"""## importing"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import math
import random
import itertools
import time

# visualization
import seaborn as sns
sns.set_style('darkgrid')

# date libraries
import datetime
import holidays
from datetime import date, timedelta, datetime

# matplotlib libraries
# %matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.dates import YearLocator, DateFormatter
import matplotlib.dates as mdates

# scipy library
from scipy import stats
from scipy.stats import norm

# sklearn library
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

# prophet library
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly, plot_cross_validation_metric
from prophet.diagnostics import performance_metrics, cross_validation

"""## defining useful functions"""

# missing data
def missing_data(input_data):
    '''
    This function returns dataframe with information about the percentage of nulls in each column and the column data type.

    input: pandas df
    output: pandas df

    '''

    total = input_data.isnull().sum()
    percent = (input_data.isnull().sum()/input_data.isnull().count()*100)
    table = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])
    types = []
    for col in input_data.columns:
        dtype = str(input_data[col].dtype)
        types.append(dtype)
    table["Types"] = types
    return(pd.DataFrame(table))

# mean absolute percentage error function
def mape(actual, pred):
    '''
    Mean Absolute Percentage Error (MAPE) Function

    input: list/series for actual values and predicted values
    output: mape value
    '''

    actual, pred = np.array(actual), np.array(pred)
    return np.mean(np.abs((actual - pred) / actual)) * 100

"""## read data"""

# datapath = 'relevant_stock_market_data2.csv'
datapath = '/content/gdrive/MyDrive/Molten Trust Limited/stock market time series data/relevant_stock_market_data2.csv'

tsdata = pd.read_csv(datapath, parse_dates=['DATE'], index_col='DATE')

tsdata.head()

min(tsdata.index), max(tsdata.index)

"""# FILTER/RESTRUCTURE"""

# filtering for agriculture stocks
filtered_stock_data = tsdata[tsdata['SECTOR'] == 'AGRICULTURE']

del filtered_stock_data['SECTOR']

filtered_stock_data.head()

filtered_stock_data.SYMBOL.unique()

# pivoting the Dataframe to the number of unique symbols
pivoted_tsdata = filtered_stock_data.pivot(columns='SYMBOL', values='CLOSEPRICE')
final_tsdata = pivoted_tsdata.rename(columns={'ELLAHLAKES': 'ELLAHLAKES', 'LIVESTOCK': 'LIVESTOCK', 'OKOMUOIL': 'OKOMUOIL',
                                              'PRESCO': 'PRESCO', 'FTNCOCOA': 'FTNCOCOA'})

# Resample the data with daily frequency by interpolation
agriculture_tsdata = final_tsdata.resample('D').interpolate(method='linear')

agriculture_tsdata.fillna(0, inplace=True)

"""# PREPROCESSING

## visualization
"""

agriculture_tsdata.tail()

del agriculture_tsdata['FTNCOCOA']

# Plotting
agriculture_tsdata.plot(figsize=(15, 6))
plt.xlabel('Year')
plt.ylabel('Stock Price')
plt.grid(True)
plt.legend(title='agriculture stocks', loc='upper left')
plt.show()

for column in agriculture_tsdata.columns:
    plt.plot(agriculture_tsdata[column])
    plt.title(column)
    plt.show()

"""## cleaning"""

missing_data(agriculture_tsdata)

"""## outliers

<b>`Data Cleaning Requirements`</b>
* remove outliers (automated by calculating and removing <b>`z-score`</b>, while Prophet helps impute those values). This will remove the outliers near 0 for all categories.
"""

# getting percentage of rows for each sector that is at 0

(agriculture_tsdata == 0).astype(int).sum(axis=0)/len(agriculture_tsdata.sum(axis=0))

"""<img src='../img/download.png' width=500px height=600px>"""

# breaking out each agricultural stocks into it's own dataframe
prediction_df_agriculture_list = []

# cleaning up dataframe using z-score to remove outliers that heavily bias the model
for column in agriculture_tsdata.columns:
    df_agriculture_clean = agriculture_tsdata[[column]].reset_index()

    z = np.abs(stats.zscore(df_agriculture_clean[column]))
    outlier_index = np.where(z > 2.7)[0] # As 99.7% of the data points lie between 3 standard deviations (Gaussian Distribution)a
    print("Dropping "+str(len(outlier_index))+" rows for following category: "+column)
    df_agriculture_clean.drop(index=outlier_index,inplace=True)
    df_agriculture_clean.set_index('DATE', inplace=True)
    prediction_df_agriculture_list.append(df_agriculture_clean)

prediction_df_agriculture_list

for i in range(len(prediction_df_agriculture_list)):
    plt.plot(prediction_df_agriculture_list[i])
    plt.title(prediction_df_agriculture_list[i].columns[0]+'- Average Daily Stocks: '+str(prediction_df_agriculture_list[i].mean()))
    plt.ylim(ymin=0)
    plt.show()

"""## sector-wise groupings (low/mid/high)"""

average_closeprices = agriculture_tsdata.apply(np.mean, axis=0).sort_values()
low, mid = np.percentile(agriculture_tsdata.apply(np.mean, axis=0).sort_values(), [25, 75])

low_vol_cols = list(average_closeprices[average_closeprices <= low].index)
mid_vol_cols = list(average_closeprices[(average_closeprices > low) & (average_closeprices < mid)].index)
high_vol_cols = list(average_closeprices[average_closeprices >= mid].index)

print(f'Low volume stocks are: {low_vol_cols}')
print(f'Middle volume stocks are: {mid_vol_cols}')
print(f'High volume stocks are: {high_vol_cols}')

# agriculture_tsdata[low_vol_cols].plot.line()
plt.plot(agriculture_tsdata[low_vol_cols])
plt.legend(low_vol_cols, loc='best', bbox_to_anchor=(1.1, 1.1))
plt.title("Low Stocks")
plt.show()

plt.plot(agriculture_tsdata[mid_vol_cols])
plt.legend(mid_vol_cols, loc='best', bbox_to_anchor=(1.1, 1.1))
plt.title("Mid-level Stocks")
plt.show()

plt.plot(agriculture_tsdata[high_vol_cols])
plt.legend(high_vol_cols, loc='best', bbox_to_anchor=(1.1, 1.1))
plt.title("High Stocks")
plt.show()

"""# MODEL BUILDING

## hyperparameter tuning

these parameters that can be tuned;

1. <b>`changepoint_prior_scale`</b>
2. <b>`seasonality_prior_scale`</b>
3. <b>`holidays_prior_scale`</b>
4. <b>`seasonality_mode`</b>
5. <b>`changepoint_range`</b>
"""

changepoint_prior_scale_range = np.linspace(0.001, 0.5, num=5).tolist()
print(changepoint_prior_scale_range)

seasonality_prior_scale_range = np.linspace(0.01, 10, num=5).tolist()
holidays_prior_scale_range = np.linspace(0.01, 10, num=5).tolist()

seasonality_mode_options = ['additive', 'multiplicative']
# changepoint_range_range = list(np.linspace(0.5, 0.95, num=5))

"""<hr>

<b>`NOTE:`</b><br> in order to determine the effective accuracy of the model, in prophet cross-validation, I have to cut off the data at various times within the dates (not including the `2003`), over a large timeframe, so as to normalize. This gives rise to the importance for <b>`cross-validation`</b>

## cross validation (prophet pipeline)

finding the <b>`"best"`</b> parameters by testing over various periods of time with those parameters (cross-validation)
"""

start_time = time.time()

dicts = {}

for feature in agriculture_tsdata.columns:

    stocksector_df = agriculture_tsdata[feature].copy().reset_index()
    stocksector_df.columns = ["ds", "y"]

    stocksector_df[["y"]] = stocksector_df[["y"]].apply(pd.to_numeric)
    stocksector_df["ds"] = pd.to_datetime(stocksector_df["ds"])

    param_grid = {
        "changepoint_prior_scale": changepoint_prior_scale_range,
        "seasonality_prior_scale": seasonality_prior_scale_range}
        #'holidays_prior_scale': holidays_prior_scale_range,
        #'seasonality_mode': seasonality_mode_options,
        #'changepoint_range': changepoint_range_range,

    # to generate all combinations of parameters
    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
    mapes = []

    # cross validation to evaluate all parameters
    for params in all_params:
      try:
        m = Prophet(**params).fit(stocksector_df)  # fit model with given params
        df_cv = cross_validation(m, initial="365.25 days", period="365.25 days", horizon = "30 days") #took parallel out
        df_p = performance_metrics(df_cv, rolling_window=1)
        # print(df_p)
        mapes.append(df_p["mape"].values[0])
      except Exception as e:
        print(f"Error for {feature}: {str(e)}")
        # Handle the error as needed, e.g., continue to the next parameter set or feature



    # Find the best parameters
    if len(mapes) > 0:
        tuning_results = pd.DataFrame(all_params)
        tuning_results["mape"] = mapes

        print(f"Results for {feature}:")
        print(tuning_results.head())

        params_dict = dict(tuning_results.sort_values("mape").reset_index(drop=True).iloc[0])
        params_dict["column"] = feature

        dicts[feature] = params_dict
    else:
        print(f"No valid results for {feature}")

print("--- %s seconds ---" % (time.time() - start_time))

"""defensie
dfas

<b>`"df_p"`</b> gives the overall MAPE, but if you want to get more granular and calculate the daily differences using the mape metric, the code is below. This allows you to detect issues in predicting certain time periods, which are inherent issues in the data that you may or may not be able to fix.
"""

#how to call params_dict for a feature

dicts

"""## feature engineering by `holidays`"""

#adding holiday data

holiday = pd.DataFrame([])

nigeria_holidays = holidays.NG()  # this is a dict
# the below is the same, but takes a string:
nigeria_holidays = holidays.country_holidays('NG')  # this is a dict


for date_, name in sorted(holidays.NG(years=[2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023]).items()):
    holiday = pd.concat([holiday, pd.DataFrame({'ds': date_, 'holiday': "NG-Holidays", 'lower_window': -2, 'upper_window': 1}, index=[0])], ignore_index=True)

holiday['ds'] = pd.to_datetime(holiday['ds'], format='%Y-%m-%d', errors='ignore')
holiday.head()

"""## backtesting with tuned metrics
`latest month up-to-date mape`
"""

prediction_days = 30
forecast_start_date = max(agriculture_tsdata.index) - timedelta(prediction_days)

"""<b>Note:</b> check a documentation here (reference to repo)!"""

# forecasted_tsdata = []

# for feature in agriculture_tsdata.columns:

#     # Prophet formats
#     df_agriculture_copy = agriculture_tsdata[feature].copy().reset_index()
#     df_agriculture_copy.columns = ['ds', 'y']
#     df_agriculture_copy[['y']] = df_agriculture_copy[['y']].apply(pd.to_numeric)
#     df_agriculture_copy['ds'] = pd.to_datetime(df_agriculture_copy['ds'])

#     df_agriculture_copy_train = df_agriculture_copy[df_agriculture_copy['ds'] < forecast_start_date]

#     # Search and get the right param_dict for this sector
#     param_dict = dicts[feature]

#     # Create the Prophet model with the correct parameters
#     model = Prophet(
#         changepoint_prior_scale=param_dict['changepoint_prior_scale'],
#         seasonality_prior_scale=param_dict['seasonality_prior_scale'],
#         holidays=holiday
#     )

#     model.fit(df_agriculture_copy_train)

#     future = model.make_future_dataframe(periods=prediction_days)
#     fcst_prophet_train = model.predict(future)  # Fixed model reference here

#     filter = fcst_prophet_train['ds'] >= forecast_start_date
#     predicted_agriculture_df = fcst_prophet_train[filter][['ds', 'yhat']]  # Use double brackets to select multiple columns

#     # Merge with the original DataFrame if needed
#     predicted_agriculture_df = predicted_agriculture_df.merge(df_agriculture_copy)

#     print(feature, mape(predicted_agriculture_df['y'], predicted_agriculture_df['yhat']))

# # Rest of your code

# PROPHET MODEL

forecasted_tsdata = []

for feature in agriculture_tsdata.columns:

    # prophet formats
    df_agriculture_copy = agriculture_tsdata[feature].copy().reset_index()
    df_agriculture_copy.columns = ['ds', 'y']
    df_agriculture_copy[['y']] = df_agriculture_copy[['y']].apply(pd.to_numeric)
    df_agriculture_copy['ds'] = pd.to_datetime(df_agriculture_copy['ds'])

    df_agriculture_copy_train = df_agriculture_copy[df_agriculture_copy['ds'] < forecast_start_date]

    # search & get the right param_dict for this sector
    param_dict = dicts[feature]

    # model
    model = Prophet(changepoint_prior_scale = dicts[feature]['changepoint_prior_scale'],
                   seasonality_prior_scale = dicts[feature]['seasonality_prior_scale'],
                   holidays = holiday)

    model.fit(df_agriculture_copy_train)

    future = model.make_future_dataframe(periods=prediction_days)
    fcst_prophet_train = m.predict(future)

    filter = fcst_prophet_train['ds'] >= forecast_start_date
    predicted_agriculture_df = fcst_prophet_train[filter][['ds', 'yhat']]
    predicted_agriculture_df = predicted_agriculture_df.merge(df_agriculture_copy)

    print(feature, mape(predicted_agriculture_df['y'], predicted_agriculture_df['yhat']))

# Check the column names in the DataFrame
print(fcst_prophet_train.columns)

"""# FINAL FORECASTING

## forecast pipeline!!!
"""

days_of_predictiion = 672
start_date_of_forecast = max(agriculture_tsdata.index)

# using prophet model technique

forecasted_dataframe = []

for feature in agriculture_tsdata.columns: # plotting for first five: for feature in agriculture_tsdata.columns[:5]:

    # formatting
    agriculture_tsdata_copy = agriculture_tsdata[feature].copy().reset_index()
    agriculture_tsdata_copy.columns = ['ds', 'y']
    agriculture_tsdata_copy[['y']] = agriculture_tsdata_copy[['y']].apply(pd.to_numeric)
    agriculture_tsdata_copy['ds'] = pd.to_datetime(agriculture_tsdata_copy['ds'])

    agriculture_tsdata_copy_train = agriculture_tsdata_copy[agriculture_tsdata_copy['ds'] < start_date_of_forecast]

    # search and get the rightparam_dicts for the sector
    param_dicts = dicts[feature]

    # model
    m = Prophet(changepoint_prior_scale = dicts[feature]['changepoint_prior_scale'],
                seasonality_prior_scale = dicts[feature]['seasonality_prior_scale'],
                holidays = holiday)

    m.fit(agriculture_tsdata_copy_train)

    future_df = m.make_future_dataframe(periods = days_of_prediction)
    forecast_prophet_train = m.predict(future_df)

    fig1 = m.plot(forecast_prophet_train)
    fig2 = m.plot_components(forecast_prophet_train)

    forecasted_df = forecast_prophet_train[forecast_prophet_train['ds'] >= start_date_of_forecast]
    forecasted_dataframe.append(forecasted_df)

"""## forecasting visualization
2023 - March to December
2024 - January to December (leap year)
"""



"""## Conclusions
<b>`...........................................................................................................`</b>
"""

